<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding">
  <meta name="keywords" content="LLM-Driven Spatial Reasoning, Open-vocabulary 3D Visual Grounding">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://zhenyangliu.github.io/">Zhenyang Liu</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="">Sixiao Zheng</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="">Siyu Chen</a><sup>4</sup>,
            </span>
            <span class="author-block">
              <a href="">Cairong Zhao</a><sup>3</sup>,
            </span>
            <span class="author-block">
              <a href="">Longfei Liang</a><sup>5</sup>,
            </span>
            <span class="author-block">
              <a href="">Xiangyang Xue</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="">Yanwei Fu</a><sup>1,2</sup>,
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Fudan University,</span>
            <span class="author-block"><sup>2</sup>Shanghai Innovation Institute,</span>
            <span class="author-block"><sup>3</sup>Tongji University,</span>
            <span class="author-block"><sup>4</sup>Zhejiang University,</span>
            <span class="author-block"><sup>5</sup>NeuHelium Co., Ltd</span>
          </div>
<!--           <p class="has-text-weight-bold has-text-dark is-size-4">CVPR 2025</p>
          <div class="column has-text-centered"> -->
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- 视频部分 -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">A Neural Representation Framework with LLM-Driven Spatial Reasoning for Open-Vocabulary 3D Visual Grounding</h2>
        <div class="content has-text-justified">
          <div class="has-text-centered" style="position: relative;">
            <video id="dp4-video" autoplay muted loop playsinline style="width: 100%; height: auto;">
              <source src="./static/videos/demo.mp4" type="video/mp4" />
              Your browser does not support the video tag.
            </video>
            <!-- 静音开关按钮 -->
            <button id="unmute-btn" 
                    style="
                      position: absolute;
                      bottom: 10px;
                      right: 10px;
                      padding: 0.5rem 1rem;
                      font-size: 0.9rem;
                      cursor: pointer;
                      background-color: rgba(0,0,0,0.5);
                      color: white;
                      border: none;
                      border-radius: 4px;
                    ">
              开启声音
            </button>
          </div>
          <!-- 视频下方的说明文字 -->
          <div style="text-align: left; max-width: 800px; margin: 1rem auto;">
            <ul>
              <li><strong><em>Empowering Language Field with Spatial Reasoning for 3D Visual Grounding</em></strong>: We overcome the limitation of language field-based open-vocabulary 3D visual grounding, which struggles to localize instances using spatial relations in language queries, by introducing a visual properties-enhanced hierarchical feature field for robust spatial reasoning and accurate grounding.</li>
              <li><strong><em>A Novel SpatialReasoner Framework</em></strong>: The proposed SpatialReasoner leverages an LLM for spatial relation decomposition, alongside a visual properties-enhanced hierarchical feature field for spatial reasoning, to ``think carefully'' and ``look carefully'', enabling accurate step-by-step localization of target instances through explicit spatial reasoning.</li>
              <li><strong><em>Outstanding Generality and Performance</em></strong>: Extensive experiments demonstrate that our method can be seamlessly integrated into diverse 3D neural representations, outperforming baseline models in 3D visual grounding and empowering their spatial reasoning capabilities.</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>

  <script>
    // 控制视频声音按钮
    const video = document.getElementById('dp4-video');
    const btn = document.getElementById('unmute-btn');

    btn.addEventListener('click', () => {
      if (video.muted) {
        video.muted = false;
        btn.textContent = '关闭声音';
      } else {
        video.muted = true;
        btn.textContent = '开启声音';
      }
      video.play();
    });
  </script>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- 使用 Flexbox 居中 -->
      <div class="has-text-centered">
        <img src="./static/images/teaser.png" alt="Teaser Image" style="width: 70%; height: auto;">
      </div>
      <h2 class="subtitle has-text-centered">
        We propose SpatialReasoner for neural representation: prior language field methods localize instances directly from complex user queries but fail to capture spatial relations in both the language query and the environment (left). Our SpatialReasoner instead utilizes a large language model (LLM) and a hierarchical feature field to think and look "step by step" (right). Crucially, reasoning through an LLM—such as spatial relation decomposition—along with hierarchical language and instance fields, allows it to "think carefully" and "look carefully" before localizing the target instance.      </h2>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
          Open-vocabulary 3D visual grounding aims to localize target objects based on free-form language queries, which is crucial for embodied AI applications such as autonomous navigation, robotics, and augmented reality. Learning 3D language fields through neural representations enables accurate understanding of 3D scenes from limited viewpoints and facilitates the localization of target objects in complex environments. However, existing language field methods struggle to accurately localize instances using spatial relations in language queries, such as ``the book on the chair.'' This limitation mainly arises from inadequate reasoning about spatial relations in both language queries and 3D scenes. In this work, we propose \textbf{SpatialReasoner}, a novel neural representation-based framework with large language model (LLM)-driven spatial reasoning that constructs a visual properties-enhanced hierarchical feature field for open-vocabulary 3D visual grounding. To enable spatial reasoning in language queries, SpatialReasoner fine-tunes an LLM to capture spatial relations and explicitly infer instructions for the target, anchor, and spatial relation. To enable spatial reasoning in 3D scenes, SpatialReasoner incorporates visual properties (opacity and color) to construct a hierarchical feature field. This field represents language and instance features using distilled CLIP features and masks extracted via the Segment Anything Model (SAM). The field is then queried using the inferred instructions in a hierarchical manner to localize the target 3D instance based on the spatial relation in the language query. Notably, SpatialReasoner is not limited to a specific 3D neural representation; it serves as a framework adaptable to various representations, such as Neural Radiance Fields (NeRF) or 3D Gaussian Splatting (3DGS). Extensive experiments show that our framework can be seamlessly integrated into different neural representations, outperforming baseline models in 3D visual grounding while empowering their spatial reasoning capability.         </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!--/ Paper video. -->
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Introduction. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">The framework of our SpatialReasoner</h2>
        <div class="content has-text-justified">
        </p>
          <!-- 图片居中 -->
          <div class="has-text-centered">
            <img src="./static/images/pipeline.png" alt="Introduction Image" style="width: 100%; height: auto;">
          </div>
          <p>
          The overall pipeline of SpatialReasoner framework. SpatialReasoner fine-tunes an LLM to decompose language queries into targets, anchors, and spatial relations. It first employs SAM to generate 2D masks for diverse instances in the training dataset. Using a neural representation model (e.g., NeRF or 3DGS) trained on multi-view images, it obtains instance scales via depth deprojection. By integrating visual properties (opacity and color) from scene reconstruction, SpatialReasoner constructs a hierarchical feature field that combines CLIP-extracted language features and mask-extraceted instance features. Reasoned instructions then query these fields to identify target and anchor candidates. By analyzing spatial relations within the query and the 3D scene, SpatialReasoner precisely localizes the referenced 3D instance. </p>
          <p>
          </p>
        </div>
      </div>
    </div>
    <!--/ Introduction. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Results -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">
          <!-- 上下排列图片 -->
          <div class="content">
            <div>
              <img src="./static/images/qual_1.png" alt="Result Image 1" style="width: 100%; height: auto; margin-bottom: 1rem;">
            </div>
            <div>
              <img src="./static/images/qual_2.png" alt="Result Image 2" style="width: 100%; height: auto;">
            </div>
          </div>
          <ul class="content">
            <li> Qualitative comparisons of spatial reasoning capability. Results demonstrate that our SpatialReasoner achieves spatial reasoning and localizing the target instance based on the spatial relation.</li>
            <li> Qualitative comparisons of 3D visual grounding capability. Results demonstrate that our SpatialReasoner achieves the superior accuracy in open-vocabulary 3D localization compared to other state-of-the-art methods.</li>
          </ul>
        </div>
      </div>
    </div>
    <!--/ Results -->
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            Theme modified from 
            <a href="https://github.com/nerfies/nerfies.github.io" target="_blank" rel="noopener noreferrer">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
